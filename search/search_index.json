{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"AIDA Data Hub Docs Documentation and user guides for services offered by     AIDA Datahub Data Science Platform <p>First login to DSP</p> <p>Logging into machine</p> Data <p>Download dataset from AIDA Nextcloud</p>"},{"location":"data/data-sharing-decider/","title":"Data sharing requests - Decider guide","text":"<p>This is a guide of how to work withe REMS (Resource Entitlement Management System) as a decider.</p> <p>Once a request for data sharing has been sent to REMS, a handler will review it. If the application looks correct, the handler then sends it to the designated decider for that dataset which is where you come in.</p> <p>The first thing you need to do as a decider is to login at https://rems.dsp.aida.scilifelab.se/</p> <p></p> <p>REMS uses Life Science Login to handle authentication. If you don't already have a user at Life Science login, please follow this guide.</p>"},{"location":"data/data-sharing-decider/#approving-applications-as-a-decider","title":"Approving applications as a Decider","text":"<p>As a decider, your task is to act as the formal approval of an application. Once you are logged in to REMS, you will have a section at the top called Actions which will show you all the open applications you are involved in. </p> <p>You will see a list of applications where the Action needed column has the text <code>Waiting for your decision</code>. These are the ones you should decide on. By clicking View you are shown the application which will display all of its details, as well as the events at the top. If the handler has left any comments, you will see them in the events list. Events have an icon which indicates whether the applicant can see them, events between the handler and you as a decider are hidden from the applicant (they can't see your decision or comments you do).</p> <p>On the right side, you can see a menu with Actions you can take for the currently displayed application.</p> <p></p> <p>You make a decision on an application by using the <code>Decide</code> action. Here you can choose to <code>Approve</code> or <code>Reject</code> the application, if you reject the application please leave a comment in the text field for the handler detailing the reason.</p> <p></p> <p>Once you have reviewed all applications waiting for your decision, the Actions section will be empty. You can look at previous applications under the show more button under Processed application</p> <p></p>"},{"location":"data/data-sharing-handler/","title":"Data sharing requests - Handler guide","text":"<p>This is a guide of how to process data sharing requests on REMS as a handler.</p> <p>Once a request for data sharing has been sent to REMS, a handler must review it. If the application looks correct, the handler then sends it to the designated decider for that dataset.</p> <p>The first thing you need to do as a handler is to login at https://rems.dsp.aida.scilifelab.se/</p> <p></p> <p>REMS uses Life Science Login to handle authentication. If you don't already have a user at Life Science login, please follow this guide.</p>"},{"location":"data/data-sharing-handler/#actions-section-listing-applications","title":"Actions section - listing applications","text":"<p>As a handler, you will have the section Actions on the top of the window. Clicking it will present you will the following view:</p> <p></p> <p>Here you can see the open applications awaiting actions. The action needed column gives a brief overview of what needs to be done. Applications listed as New application will need a handler to review it. Open the application by clicking <code>view</code>.</p>"},{"location":"data/data-sharing-handler/#the-application-view","title":"The Application View","text":"<p>You will be shown the application as filled in by the requester:</p> <p></p> <p>At the top is a list of the events for this application. The events which the applicant sees have an open eye icon, while events hidden (such as requests for decision) are denoted by a crossed out eye. To the top right are the actions you as a handler can perform. The ones you will use are:</p> <ul> <li><code>Return to applicant...</code></li> <li><code>Request decision</code></li> <li><code>Approve or reject</code></li> </ul> <p>The important details of the application is at the bottom:</p> <p></p> <p>These lists the details the applicant has given about the recipient scientist and is what you as a handler will use to review the application.</p>"},{"location":"data/data-sharing-handler/#reviewing-the-application","title":"Reviewing the application","text":"<p>Most datasets require us to establish that the recipient scientist is trained in handling sensitive data. As a way of proving this, we have put a requirement on the recipient scientist to hold at least a PhD in a relevant field. As a handler you need to look at the fields filled in by the applicant, follow the given links and information and make a conclusion that the recipient scientist has the required credentials. We also need it to be clearly established that the recipient scientist is connected to the institution indicated in the application, meaning that their email address is their institutional one and that there is some profile page or verified ORCID connection to it.</p> <ul> <li>If the application does not clearly show the above, follow the instructions under Application is incomplete and should not be ammended if this application would never be approved (e.g. from a region we're not allowed to share with) or Application is incomplete but can be ammended if the application is just missing enough information.</li> <li>If everything looks good, instead follow the instructions under Application is complete.</li> </ul> <p>Often, the application will have multiple members invited. To process an application, only the dataset recipient needs to have logged in and approved of the agreement. The status of other invited members is not a blocker for reviewing the application.</p>"},{"location":"data/data-sharing-handler/#application-is-incomplete-and-should-not-be-ammended","title":"Application is incomplete and should not be ammended","text":"<p>In the case that the application is very incomplete or the dataset cannot be shared due to e.g. policy, use the <code>reject</code> option under the <code>Approve or reject...</code> action with a comment detailing why the application is rejected. This comment will be visible to the applicant.</p>"},{"location":"data/data-sharing-handler/#application-is-incomplete-but-can-be-ammended","title":"Application is incomplete but can be ammended","text":"<p>If you cannot with reasonable effort establish that the recipient scientist is competent to handle medical data, return the application (use the action Return to applicant) to the requester with the comment:</p> <pre><code>Thank you for your interest in this dataset. We could not establish the identity of the recipient scientist with the given information. Please read https://docs.datahub.aida.scilifelab.se/data/data-sharing-request/#rejected-applications for further details. Please clarify the following:\n\n- Name of recipient scientist,\n- Recipient scientist e-mail for the workplace institution,\n- Name and department of recipient scientist institution,\n- Address of institution,\n- Details which demonstrate that the researcher has expertise in relevant field, such as ORCID, profile page at research institution\n</code></pre> <p>Remove the bullet points which are clear and don't need further details.</p> <p></p>"},{"location":"data/data-sharing-handler/#application-is-complete","title":"Application is complete","text":"<p>If the details of the recipient scientist look correct and suitable, the next step is to send it to the decider. To do this, use the <code>Remark...</code> action to add a trigger text which the automated system will pick up on. This gives you a text field where you can enter a remark (by default not visible to the applicant). Use the string <code>send to deciders</code> to trigger the automatic sending. Note that the spelling must be exact, incorrect strings will be silently ignored.</p> <p></p> <p>After the automated script has been run (every 5 minutes) the deciders will have been added to the application. The automated script will re-add deciders who have not answered within 24 hours to trigger a reminder.</p> <p>If you need to re-request decisions from all deciders (e.g. for a resubmitted application) you can add the <code>send to deciders</code> remark again.</p>"},{"location":"data/data-sharing-handler/#track-application-status","title":"Track application status","text":"<p>You can view the status of the application in the Actions section, where the status will be \"Waiting for decision\"</p> <p></p> <p>If you need details about what decider you are waiting on (in the case of multiple deciders) you can go into the application (via the <code>View</code> button) and look at the Events list:</p> <p></p>"},{"location":"data/data-sharing-handler/#approving-the-application","title":"Approving the application","text":"<p>Once all deciders have made their decisions on an application, it will be listed under the Actions with the text <code>All requests have been responded to</code>.</p> <p></p> <p>Once all deciders have responded, review the application again</p> <p></p> <p>Under the Events, you can see what the decisions are. If the deciders don't all approve the application, they will have given a comment with the rejection detailing the reason.</p> <p>Answer the applicant either with a <code>rejection</code> if the deciders comment is a hard reject or <code>Return to applicant...</code> if the reason is more about unclear details. In both cases reformulate the reason for rejection from the decider for the applicant to act on.</p> <p>If all deciders approve the application, you should now finalize the approval. Use the <code>Approve or reject...</code> action with the text below as a comment (will be visible to the applicant). Note that for datasets where there is no recipient scientist (e.g. datasets for educational purpose) replace \"Recipient Scientist\" above with \"Applicant\".</p> <pre><code>Your application has now been approved. We have prepared a data export for you. An e-mail with a temporary share link and password will be sent to the Recipient Scientist. This link is valid for 14 days.\n\nWe recommend rclone for downloading data (https://docs.datahub.aida.scilifelab.se/data/download-dataset). Best of luck with the data!\n</code></pre> <p></p> <p>Once you've filled in the comment, approve the applications and you're done! After the management script has been run (every 5 minutes), the link will have been created and sent to the applicants email address. You can see the events detailing this in the events section.</p> <p></p>"},{"location":"data/data-sharing-handler/#automatic-creating-of-share-links","title":"Automatic creating of share links","text":"<p>The automated application managment system will create a link and password for the NextCloud repository and automatically send them via email to the recipient scientist (or applicant). This link is valid for 14 days.</p> <p>If you need to trigger the automated sharing system again, use the <code>Remark action...</code> and enter the string <code>reshare</code> which will trigger a new sharing of the dataset to the recipient scientist as above.</p> <p></p> <p>After the management script has run (every 5 minutes), the data will have been reshared with the same recipient as the original share.</p> <p></p>"},{"location":"data/data-sharing-request/","title":"Requesting access to AIDA Data Hub dataset through REMS","text":"<p>Most datasets on the AIDA Data Hub use REMS (Resource Entitlement Management System) to manage requests for and access to datasets. This system allows you to apply for access to datasets and for handlers to review your application. Before you request access to a dataset, you must first login at https://rems.dsp.aida.scilifelab.se/</p> <p></p> <p>To login to REMS, you need to authenticate using Life Science Login. If you don't already have a user at Life Science login, please follow this guide.</p>"},{"location":"data/data-sharing-request/#using-rems","title":"Using REMS","text":"<p>Once you have authenticated using Life Science login, you will be redirected back to REMS, you will see the dataset catalogue:</p> <p></p> <p>Here you can browse all available datasets and add them to a cart. Each application still needs to be done one at a time, but the cart allows you to collect datasets of interest before beginning the application process. After you've added a dataset to the basket, you will have the option to apply for access:</p> <p></p>"},{"location":"data/data-sharing-request/#applying-for-access","title":"Applying for access","text":"<p>To apply for a dataset, click the <code>Apply</code> button. This will take you to the application form.</p> <p></p> <p>This form has multiple fields which needs to be filled in. All datasets have some form of agreement you need to accept. Often, these are in the form of PDF attachment which you need to download and review before agreeing.</p> <p>Many datasets also have mandatory fields to demonstrate that the recipient researcher is qualified to conduct ethical research using medical data. Details about the researcher are required for us to lawfully share the data. For further details, see the section Legal explanation.</p> <p>At the top right of the application form, you will find a set of available actions:</p> <ul> <li>Save as draft: Save the current state of your application without submitting it. You can continue working on it later.</li> <li>Send application: Submit the application for processing. It will appear in your list of applications.</li> <li>Delete draft: Delete the application, removing it from your list of applications.</li> <li>Copy as new application: Create a new application for the same dataset, with the same filled-in details.</li> <li>Download PDF: Download a PDF version of the application.</li> </ul> <p>Once you have filled in the application, use the Send application action to submit it for processing. A handler will manually review your application. Once the review is complete, you will receive a decision, either approval or rejection.</p>"},{"location":"data/data-sharing-request/#approved-applications","title":"Approved applications","text":"<p>If your application is approved, you will receive an email with instructions on how to access the dataset. Please note that download credentials are time-limited, so be sure to use them promptly.</p>"},{"location":"data/data-sharing-request/#rejected-applications","title":"Rejected applications","text":"<p>A rejection usually means that the handler could not, with reasonable effort, verify that the recipient researcher meets the legal requirements to receive the data (see Legal explanation). If your application is rejected due to insufficient information, you are welcome to submit a new one, but please ensure that:</p> <ul> <li>The recipient researcher holds at least a PhD in a relevant field.</li> <li>The researcher's email address is clearly associated with the stated institution.</li> <li>The researcher's profile page and other supporting links clearly demonstrate competence in conducting ethical medical research in a relevant field.</li> </ul> <p>The clearer the researcher\u2019s credentials are (e.g., an institutional email address, a detailed institutional profile page, and a publication record or position relevant to the field), the easier it will be for the handler to process the application.</p>"},{"location":"data/data-sharing-request/#the-application-listing","title":"The Application listing","text":"<p>You can see the status of your applications in REMS under the Applications menu:</p> <p></p> <p>You can continue working on your saved drafts, as well as review your submitted applications.</p>"},{"location":"data/data-sharing-request/#legal-explanation","title":"Legal explanation","text":"<p>As a policy, we require the recipient researcher to hold at least a PhD in a relevant field. This requirement has to do with the legal framework under which the data can be shared.</p> <p>This requirement is based on the legal framework governing data sharing. According to Swedish law, clinical datasets can be shared only when used for research activities conducted under the supervision of a qualified researcher, someone with the necessary scientific competence to ensure that research is carried out as described in the ethical review application and in accordance with institutional policies.</p> <p>For a more detailed discussion of the legal framework surrounding the sharing of clinical data in Sweden, please see the Legal Discussion on the AIDA Data Hub.</p>"},{"location":"data/download-dataset/","title":"Download a dataset from AIDA Nextcloud using rclone","text":""},{"location":"data/download-dataset/#step-1-setup-a-remote","title":"Step 1 - Setup a remote","text":"<pre><code>rclone config create drsk webdav url=https://nextcloud.aida.scilifelab.se/public.php/webdav vendor=nextcloud user=&lt;USERNAME&gt; pass=&lt;PASSWORD&gt;\n</code></pre> <p>Alternatively set vendor to other, if the above command is failing when set to nextcloud.</p> <pre><code>rclone config create drsk webdav url=https://nextcloud.aida.scilifelab.se/public.php/webdav vendor=other user=&lt;USERNAME&gt; pass=&lt;PASSWORD&gt;\n</code></pre> <p>Here, we have chosen the name <code>drsk</code> for the remote. You can of course choose whatever name you want. Probably use something short that doesn't contain spaces.</p> <p>Your username is the last part of the share link you got from us, i.e: <code>https://nextcloud.aida.scilifelab.se/s/USERNAME</code></p> <p>Your username could for example be something like <code>Y4gVyiVyyiF6fug</code>.</p> <p>Your password is the string of random characters you got from us separately. You will probably need to \"quote\" or 'quote' it on the command-line, as it very likely contains special characters.</p> <p>If you'd rather answer questions interactively, do this instead, and do what it says:</p> <pre><code>rclone config`\n</code></pre> <p>The exact questions may differ depending on what version rclone you have.</p>"},{"location":"data/download-dataset/#step-2-download-from-the-remote","title":"Step 2 - Download from the remote","text":"<p>Use the same name here that you chose in the previous step.</p> <pre><code>rclone copy drsk: .\n</code></pre> <p>Or if you want progress written to the terminal:</p> <pre><code>rclone copy --progress drsk: .\n</code></pre>"},{"location":"data/working-with-dicom/","title":"Working with DICOM","text":"<p>Digital Imaging and Communications in Medicine (DICOM) is the international standard to transmit, store, retrieve, print, process, and display medical imaging information. It is prevalent in radiology and is gaining traction also in pathology.</p> <p>Some datasets on the AIDA Data Hub are provided in DICOM format. In DICOM, data is provided in nested and interlinking structures, which can be nontrivial for newcomers to the field to navigate.</p> <p>For an introduction to how one can work with DICOM data, AIDA is providing a Jupyter notebook, which is publicly available through the SciLifeLab/NBIS GitHub organization:</p> <p>https://github.com/NBISweden/dicom-data-visualizer</p>"},{"location":"dsp/advanced/custom-proxy/","title":"Setting up a custom HTTP Proxy","text":"<p>By default, the Virtual Machines (VMs) on the Data Science Platform (DSP) can make HTTP requests to the outside world only by going through an inspecting proxy. To make the default configuration of DSP secure, this proxy has a strict list of allowed resources that internal machines can access. For some projects, the default is too strict, and one way of getting around it is to set up your own proxy for your VMs.</p> <p>But do note that before doing so you must verify that it follows the overall security guidelines for the current research project.</p> <p>In this guide, we will set up a different proxy on a VM using <code>privoxy</code> and tunnel HTTP requests through the connecting machine\u00b4s SSH connection.</p> <p>As a curiosity, this builds a lot on functionality offered by OpenSSH and the privoxy bits can in many cases be bypassed by referencing the SOCKS5 proxy OpenSSH provides directly, e.g. by <code>http_proxy=socsk5h://127.0.0.1:3128</code>. But while it has fairly wide support it's not universal, and adding privoxy to serve as a http proxy will help with some of those - but if you just need a quick <code>apt install</code> or similar, you can bypass that and use the socks proxy directly.</p> <p>Note that this will circumvent strict filtering and essentially allow any HTTP request from your VM to be carried out (unless you explicitly add filtering on your own machine or network).</p>"},{"location":"dsp/advanced/custom-proxy/#set-up-privoxy-on-the-vm","title":"Set up <code>privoxy</code> on the VM","text":"<p><code>privoxy</code> is a simple web proxy which is provided in the base Ubuntu repositories, this means that it will be easy to install it on Ubuntu-based VMs on DSP since the default package repositories are in our allow lists. Install it on your VM by running:</p> <pre><code>sudo apt install privoxy\n</code></pre> <p>Now we need to configure it to forward requests using SOCKS5:</p> <pre><code>echo 'forward-socks5  /  127.0.0.1:3128  .' | sudo tee -a /etc/privoxy/config\n</code></pre> <p>Restart <code>privoxy</code> to read the changes:</p> <pre><code>sudo systemctl restart privoxy\n</code></pre> <p>Now disconnect from the VM (we need to add additional configurations to the SSH client)</p> <pre><code>exit\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#configure-ssh","title":"Configure SSH","text":"<p>Above, we configured <code>privoxy</code> to forward HTTP requests to a SOCKS5 proxy on the port <code>3128</code> on the machine where it\u00b4s running (i.e. the VM). We'll now make our own connecting machine relaying the HTTP requests by telling our SSH client to listen to this port on the remote host (VM) and tunnel traffic to our connecting machine. We do this by using a \"remote forward\" connection. You can do this \"on-the-fly\" when you connect to your machine using the flag <code>-R 3128</code>, but it's more convenient to add this to your SSH config:</p> <pre><code>Host dsp-project\n  HostName [VM_FLOATING_IP]\n  User ubuntu\n  ProxyJump dsp\n  ServerAliveInterval 10\n  RemoteForward 3128 localhost:3128\n\nHost dsp\n  Hostname dsp.aida.scilifelab.se\n  User [MY_EMAIL]\n</code></pre> <p>The <code>RemoteForward 3128</code> is what sets up the reverse tunnel.</p> <p>Now when you connect to your VM over SSH (e.g. <code>ssh dsp-project</code> in config example above), SSH will forward any traffic connecting to the port 3128 on the VM to our connecting machine and carry out the HTTP requests.</p>"},{"location":"dsp/advanced/custom-proxy/#configure-remote-programs","title":"Configure remote programs","text":"<p>By default, the VM is configured to use the DSP HTTP proxy. This is set up using the environmental variables <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code>, <code>http_proxy</code> and <code>https_proxy</code>. We'll replace their values with the address of our <code>privoxy</code> instance instead. By default it listens to port 8118 for requests, so we'll set our environmental variables to this:</p> <pre><code>export http_proxy=http://127.0.0.1:8118 https_proxy=http://127.0.0.1:8118\nexport HTTP_PROXY=http://127.0.0.1:8118 HTTPS_PROXY=http://127.0.0.1:8118\n</code></pre> <p>This changes the variables for the currently running shell (so just for your current session). Once you have exported the variables, you should be able to test that the proxy works by running (in the same shell):</p> <pre><code>curl -v --proxy http://127.0.0.1:8118 https://example.com\n</code></pre> <p>Which should output an HTML document to you terminal; the HTMLlanding page of <code>example.com</code>.</p>"},{"location":"dsp/advanced/custom-proxy/#creating-alias-for-temporarily-switching-proxy","title":"Creating alias for temporarily switching proxy","text":"<p>We can add a shell alias if we want to temporarily set the environmental variables:</p> <pre><code>alias proxy='http_proxy=http://127.0.0.1:8118 https_proxy=http://127.0.0.1:8118 HTTP_PROXY=http://127.0.0.1:8118 HTTPS_PROXY=http://127.0.0.1:8118'\n</code></pre> <p>Then use it for specific commands:</p> <pre><code>proxy pip3 install numpy   # Uses proxy\npip3 install numpy         # Does not use proxy\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#making-proxy-settings-persistent","title":"Making Proxy Settings Persistent","text":"<p>The above exports of environmental variables only affect the shell you run it in, and you might want your proxy to be used throughout your account. To achieve that run the following:</p> <pre><code>cat &gt;&gt; ~/.bashrc &lt;&lt; EOF\n\n# Proxy settings\nexport http_proxy=http://127.0.0.1:8118\nexport https_proxy=http://127.0.0.1:8118\nexport HTTP_PROXY=http://127.0.0.1:8118\nexport HTTPS_PROXY=http://127.0.0.1:8118\nEOF\n</code></pre> <p>Then reload your Bash configuration:</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#disabling-privoxy","title":"Disabling Privoxy","text":"<p>To remove the persistent settings from your <code>.bashrc</code>:</p> <pre><code>sed -i \\\n    -e '/^# Proxy settings$/d' \\\n    -e '/^export http_proxy=http:\\/\\/127\\.0\\.0\\.1:8118$/d' \\\n    -e '/^export https_proxy=http:\\/\\/127\\.0\\.0\\.1:8118$/d' \\\n    -e '/^export HTTP_PROXY=http:\\/\\/127\\.0\\.0\\.1:8118$/d' \\\n    -e '/^export HTTPS_PROXY=http:\\/\\/127\\.0\\.0\\.1:8118$/d' \\\n    ~/.bashrc\n</code></pre> <p>To temporarily stop Privoxy:</p> <pre><code>sudo systemctl stop privoxy\n</code></pre> <p>To disable Privoxy from starting on boot:</p> <pre><code>sudo systemctl disable privoxy\n</code></pre> <p>To completely remove Privoxy:</p> <pre><code>sudo apt remove privoxy\nsudo apt autoremove\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#configuring-apt-to-use-privoxy","title":"Configuring APT to Use Privoxy","text":"<p>While the default sources for APT are allowed in the DSP proxy, you might have added additional package repositories, in which case they will likely be blocked. You can tell APT to use privoxy instead by adding a proxy configuration file:</p> <ol> <li>Create an APT configuration file for the proxy:</li> </ol> <p><code>shell    sudo bash -c 'cat &gt; /etc/apt/apt.conf.d/80proxy &lt;&lt; EOF    Acquire::http::Proxy \"http://127.0.0.1:8118\";    Acquire::https::Proxy \"http://127.0.0.1:8118\";    EOF'</code></p> <ol> <li>Verify the configuration:</li> </ol> <p><code>shell    cat /etc/apt/apt.conf.d/80proxy</code></p> <ol> <li>Test APT with the proxy:</li> </ol> <p><code>shell    sudo apt update</code></p>"},{"location":"dsp/advanced/custom-proxy/#docker-proxy-configuration","title":"Docker Proxy Configuration","text":""},{"location":"dsp/advanced/custom-proxy/#system-wide-docker-proxy-method-1","title":"System-Wide Docker Proxy (Method 1)","text":"<ol> <li>Create or edit the Docker service configuration file:</li> </ol> <p><code>shell    sudo mkdir -p /etc/systemd/system/docker.service.d/    sudo bash -c 'cat &gt; /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt; EOF    [Service]    Environment=\"HTTP_PROXY=http://127.0.0.1:8118\"    Environment=\"HTTPS_PROXY=http://127.0.0.1:8118\"    Environment=\"NO_PROXY=localhost,127.0.0.1,::1\"    Environment=\"no_proxy=localhost,127.0.0.1,::1\"    EOF'</code></p> <ol> <li>Reload the Docker daemon configuration:</li> </ol> <p><code>shell    sudo systemctl daemon-reload    sudo systemctl restart docker</code></p> <ol> <li>Verify the Docker proxy settings:</li> </ol> <p><code>shell    sudo systemctl show --property=Environment docker</code></p>"},{"location":"dsp/advanced/custom-proxy/#docker-client-configuration-method-2","title":"Docker Client Configuration (Method 2)","text":"<p>For a user-specific Docker configuration:</p> <pre><code>mkdir -p ~/.docker\ncat &gt; ~/.docker/config.json &lt;&lt; EOF\n{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://127.0.0.1:8118\",\n     \"httpsProxy\": \"http://127.0.0.1:8118\",\n     \"noProxy\": \"localhost,127.0.0.1,::1\"\n   }\n }\n}\nEOF\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#docker-build-and-run-with-proxy","title":"Docker Build and Run with Proxy","text":"<p>To make your proxy available during Docker build:</p> <pre><code>docker build --network=host .\n</code></pre> <p>To run a container with host network (and thus access to your proxy):</p> <pre><code>docker run -it --rm --network host ubuntu bash\n</code></pre>"},{"location":"dsp/advanced/custom-proxy/#using-proxy-inside-docker-container","title":"Using Proxy Inside Docker Container","text":"<p>Inside the container, set up proxy configuration the same way as we've done above in this guide:</p> <pre><code># Create an alias for temporary proxy usage\nalias proxy='http_proxy=http://127.0.0.1:8118 https_proxy=http://127.0.0.1:8118 HTTP_PROXY=http://127.0.0.1:8118 HTTPS_PROXY=http://127.0.0.1:8118'\n\n# Use proxy for specific commands\nproxy pip3 install numpy   # Uses proxy\npip3 install numpy         # Does not use proxy\n\n# Or set for the entire session\nexport http_proxy=http://127.0.0.1:8118 https_proxy=http://127.0.0.1:8118\nexport HTTP_PROXY=http://127.0.0.1:8118 HTTPS_PROXY=http://127.0.0.1:8118\npip3 install numpy         # Now uses proxy\n</code></pre>"},{"location":"dsp/advanced/email/","title":"Sending emails from DSP","text":"<p>DSP supports email deliveries but to limit exfiltration possibilities, there are additional steps that need to be done to make it work.</p>"},{"location":"dsp/advanced/email/#how-it-works","title":"How it works","text":"<p>A list of allowed outgoing addresses can be registered for a secure environment. These are then possible to use as sender for outgoing emails. Emails with a different sender will be rejected.</p>"},{"location":"dsp/advanced/email/#requirements","title":"Requirements","text":"<p>To support outgoing emails, you'll need to provide</p> <ul> <li>a list of email addresses that will be used as sender addresses</li> <li>for each such outgoing address, the outgoing email details to use:</li> <li>server</li> <li>credentials (username and password) that are allowed to send emails     with the outgoing address</li> </ul> <p>We strongly encourage using separate accounts.</p>"},{"location":"dsp/advanced/email/#setup","title":"Setup","text":"<p>Currently this requires manual interaction, contact DSP support to set things up.</p>"},{"location":"dsp/advanced/email/#use","title":"Use","text":"<p>Once you have confirmation that the setup is done, you can use 10.253.254.250 as SMTP server. You do NOT need to provide authentication but we strongly encourage using encryption (TLS) for these connection, either using <code>smtps</code> or SMTP with <code>STARTTLS</code>.</p>"},{"location":"dsp/advanced/remote-access/","title":"Remote cloud access","text":"<p>Remote control is available for API usage and command line usage.</p> <p>DSP uses OpenStack and can be controlled with the python openstack client.</p>"},{"location":"dsp/advanced/remote-access/#getting-the-client","title":"Getting the client","text":"<p>If it's not available natively in your environment (e.g. Ubuntu has a <code>python3-openstackclient</code> package), you can install the client from standard sources, typically this will look like:</p> <pre><code>user@host:~$ python3 -mvenv openstackenvironment\nuser@host:~$ . openstackenvironment/bin/activate\n(openstackenvironment) user@host:~$ pip install python-openstackclient\nCollecting python-openstackclient\n[..]\nSuccessfully installed [..]\n</code></pre>"},{"location":"dsp/advanced/remote-access/#application-credentials","title":"Application credentials","text":"<p>To access the API, you need to use application credentials, available in OpenStack Horizon under Identity -&gt; Application Credentials.</p> <p>From that panel, you can request the creation of a new application credential by clicking the button \"Create Application Credential\".</p> <p></p> <p>Clicking it will open the dialog to create a new set of application credentials.</p> <p></p> <p>You must give your application credential a name and can optionally enter a descripition.</p> <p>Once created, the application will be protected by a secret. This can be provided by your or generated automatically (for many uses you do not need to manage this directly).</p> <p>If desired, it can be restricted how the application can be used (what operations it's allowed for). It's also possible to determine when it should expires. We recommend setting an expiration date no more than a year in the future.</p> <p>[!IMPORTANT] For security reasons, we will start enforcing a finite life span for application credentials. The exact details of this is still to be determined, though.</p> <p>If you press the \"Create Application Credential\" button, the credential will be created and shown. If you didn't specify a secret, the generated secret will also be shown.</p> <p></p> <p>For most uses, downloading the <code>clouds.yaml</code> or <code>openrc</code> file will be suitable and will contain the information needed to use the secret.</p> <p>The application credentials panel also allows you to delete an application credential if it's no longer needed.</p>"},{"location":"dsp/advanced/remote-access/#connecting","title":"Connecting","text":"<p>Assuming the client is installed in <code>PATH</code> per above and there is a downloaded openrc file, it can be used as such:</p> <pre><code>user@host:~$ . ~/path/to/downloaded-openrc.sh\nuser@host:~$ openstack server list\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n| ID                                   | Name   | Status | Networks                              | Image                    | Flavor              |\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n| f353bdf8-9ab1-4b2b-b75c-4e0ab0d4c28b | test | ACTIVE | aidadatahubtest-net=10.36.138.141       | N/A (booted from volume) | verdi-sixteenthnode |\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n</code></pre> <p>it's also possible to use the YAML file:</p> <pre><code>user@host:~$ OS_CLIENT_CONFIG_FILE=~/path/to/downloaded-openrc.sh\nuser@host:~$ export OS_CLIENT_CONFIG_FILE\nuser@host:~$ openstack server list\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n| ID                                   | Name   | Status | Networks                              | Image                    | Flavor              |\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n| f353bdf8-9ab1-4b2b-b75c-4e0ab0d4c28b | test | ACTIVE | aidadatahubtest-net=10.36.138.141       | N/A (booted from volume) | verdi-sixteenthnode |\n+--------------------------------------+--------+--------+---------------------------------------+--------------------------+---------------------+\n</code></pre>"},{"location":"dsp/examples/gpu-iaas/","title":"GPU IaaS with custom software and data","text":"<p>Infrastructure as a Service (IaaS) provides resources that you manage yourself. It is a service for advanced customers that provides a lot of freedom to those with the skills and responsibility to handle it.</p>"},{"location":"dsp/examples/gpu-iaas/#topics","title":"Topics","text":"<p>In this example you, as a Customer lead, will go through the steps of installing software and uploading data to a GPU virtual machine you create on the AIDA Data Hub Data Science Platform (DSP) for sensitive data.</p> <p>This example assumes experience with Linux, and authority to initiate expense.</p>"},{"location":"dsp/examples/gpu-iaas/#instructions","title":"Instructions","text":""},{"location":"dsp/examples/gpu-iaas/#1-launch-a-gpu-enabled-virtual-machine","title":"1. Launch a GPU enabled virtual machine","text":"<ol> <li>Visit the DSP Horizon customer self-service portal at https://dsp.aida.scilifelab.se/</li> <li>Log in using your DSP Horizon credentials.</li> <li>Pick the correct secure environment from the project selector drop down menu    top left.</li> <li>Add your SSH public key to your project by clicking Project &gt; Compute &gt;    Key Pairs, then Import Public Key.</li> <li>Create a GPU enabled virtual machine by clicking Instances &gt; Launch instance,    and</li> <li>In Details, Instance name: Put a good name for a compute server,       like \"Jupyter demo\".</li> <li>In Source, click the up arrow icon next to an image that has Docker,       CUDA, Miniforge, Jupyter Lab and RMD.</li> <li>In Flavor, click the up arrow icon next to a flavor that has GPU.       Bigger is more expensive.</li> <li>If applicable: In Security Groups, click the up arrow icon next to       <code>incoming</code>.</li> <li>In Key Pair, verify that your key is allocated.</li> <li>Click Launch instance.</li> <li>Click Associate Floating IP &gt; IP Address &gt; pick one, fill in in next step.</li> <li>Wait for Power State to become Running.</li> </ol>"},{"location":"dsp/examples/gpu-iaas/#2-configure-your-computer-for-easy-access","title":"2. Configure your computer for easy access","text":"<p>Add to SSH-config (eg <code>~/.ssh/config</code>):</p> <pre><code>Host jupyter-demo\n  HostName [associated IP in Horizon]\n  User ubuntu\n  ProxyJump dspgateway\n  ServerAliveInterval 10\n  LocalForward 8888 localhost:8888\n  LocalForward 6006 localhost:6006\n  LocalForward 5901 localhost:5901\n\nHost dspgateway\n  HostName dsp.aida.scilifelab.se\n  User [Identity in LifeScience Login]\n</code></pre> <p>This sets up your computer to use the DSP SSH access gateway when making SSH connections to your VM. By default, DSP rejects SSH connections that are not made through the SSH access gateway.</p> <p>The configuration has two <code>Host</code> sections, one for the Virtual Machine you created (jupyter-demo) and one for the gateway (dspgateway) which you will use to \"jump\" into the secure environment.</p> <p>The <code>ProxyJump</code> command in the jupyter-demo section tells SSH that it should connect to the VM by jumping through the host <code>dspgateway</code>. SSH authentication to this gateway is done using Life Science Login, which is the default authentication method for the DSP. To log accesses and match them to the correct login account identity, your email address should be set as the <code>User</code>, replacing the placeholder [Identity in LifeScience Login].</p> <p><code>ServerAliveInterval</code> makes it easier to maintain a connection, and to detect when it has gone stale.</p> <p>The <code>LocalForwards</code> define SSH secured port forwards. They connect ports on your computer with ports on your VM in the secure environment. They allow you to work with Jupyter notebooks, TensorBoard, and VNC remote desktop running on the VM in your secure environment as if though they were running on your computer.</p>"},{"location":"dsp/examples/gpu-iaas/#3-install-software-from-public-repositories-that-are-trusted-by-the-platform","title":"3. Install software from public repositories that are trusted by the platform","text":"<p>DSP secure environments block connections by default. However, DSP provides an inspecting http proxy that enables downloading software and security updates from public repositories that are trusted by AIDA Data Hub. DSP data science images are preconfigured to make transparent use of this proxy, as demonstrated in this next step.</p> <p>Here, we clone the Jupiter notebook GitHub repository and use apt and pip to install its dependencies in a Python virtual environment. We do this inside a tmux virtual terminal so that work is kept persistent, so that running processes are not killed if connection is lost.</p> <pre><code>ssh jupyter-demo\ntmux\ngit clone https://github.com/eryl/aida-transformers-workshop-code.git\ncd aida-transformers-workshop-code\nsudo apt update\nsudo apt install python3-venv\npython3 -m venv .venv\nsource .venv/bin/activate\nexport REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt # Trust DSP CA\npip install -r requirements.txt\nsudo docker pull alpine # just for show, we don't really need Alpine nor Docker for this :)\n</code></pre> <p>Note: The restrictivity of the DSP inspecting http proxy is continually updated, to adjust to updates in public repositories that make them more or less appropriate for secure environments. For example, publicly accessible granular download counters are increasingly popular despite constituting a trivially exploitable data exfiltration method.</p>"},{"location":"dsp/examples/gpu-iaas/#4-upload-own-data","title":"4. Upload own data","text":"<ol> <li>Download demo data to your own computer, and upload it to your VM:</li> </ol> <pre><code>cd ~/Downloads\nwget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\nwget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\nrsync --progress {annotations,images}.tar.gz jupyter-demo:\n# In case you don't have rsync locally, you can use scp:\n# scp {annotations,images}.tar.gz jupyter-demo:\n</code></pre> <ol> <li>Put demo data where our Jupyter notebook expects it</li> </ol> <pre><code>ssh jupyter-demo\nmkdir -p ~/aida-transformers-workshop-code/datasets/oxfordiiipet/oxford-iiit-pet\ncd ~/aida-transformers-workshop-code/datasets/oxfordiiipet/oxford-iiit-pet\ntar xf ~/annotations.tar.gz\ntar xf ~/images.tar.gz\n</code></pre>"},{"location":"dsp/examples/gpu-iaas/#5-inspect-data-in-a-remote-desktop","title":"5. Inspect data in a remote desktop","text":"<ol> <li>Start a VNC server on your VM</li> </ol> <pre><code>sudo chown -R ubuntu:ubuntu ~/.vnc\ntightvncserver -nolisten tcp -localhost :1\n</code></pre> <p>This starts a TightVNC server on the node. We also tell it to only listen to TCP connections, and only those coming from localhost (this means other computers in the same private network can't connect to the VNC server by default).</p> <ol> <li>On your computer, point your VNC client of choice to <code>localhost:5901</code> to    connect through the SSH port forward that you set up in step 2. You can for    example use Remmina, which comes preinstalled on Ubuntu.</li> </ol> <p>Note: In the future, AIDA Data Hub will provide ways to connect to a remote desktop in a secure environment which do not require the user to have server administrator skills.</p>"},{"location":"dsp/examples/gpu-iaas/#6-use-a-jupyter-notebook-to-train-an-ai-model-and-monitor-progress-graphically","title":"6. Use a Jupyter notebook to train an AI model, and monitor progress graphically","text":"<ol> <li>Connect to your VM and start up the demo Jupyter notebook</li> </ol> <pre><code>ssh jupyter-demo\ncd ~/aida-transformers-workshop-code\nsource .venv/bin/activate\ncd notebooks\n# no pw/token, don't open a browser, only allow connections from localhost:\njupyter notebook --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.open_browser=False --NotebookApp.ip='127.0.0.1'\n</code></pre> <p>Your Jupiter notebook is now ready to use, as long as you have this SSH connection and its port forwards open.</p> <ol> <li>Using a web browser on your computer, visit    http://127.0.0.1:8888 to connect to your Jupyter    notebook through the SSH port forward that you set up in step 2. Without it, you    will not be able to connect.</li> <li>Choose transformers_for_images.ipynb</li> <li>Use Shift+Enter to run the cells manually in sequence. Edit if you like. You    are now training AI models on GPU enabled IaaS compute resources in a secure    environment on the AIDA Data Hub Data Science Platform.</li> <li>Optional: Notebook step ~23 creates a TensorBoard interface, which can be    used to monitor training progress graphically. It will initially be empty, but    as subsequent training progresses, you can use the Refresh icon in TensorBoard    interface to read and visualize current state graphically. The TensorBoard    interface uses the second SSH port forward that you set up in step 2 and cannot    connect without it.</li> </ol>"},{"location":"dsp/getting-started/basic-setup-network/","title":"Basic setup for new VMs to be able to access external resources","text":""},{"location":"dsp/getting-started/basic-setup-network/#the-short-bit","title":"The short bit","text":"<p>We offer a service at http://10.253.254.250/ which serves a script with suggested configuration. When connected to a newly deployed machine you can do the e.g. the below to configure it.</p> <pre><code>\n# Fetch the script\ncurl http://10.253.254.250/ &gt; dspconfigscript\n# Inspect and see that you don't disagree with any of the configurations\n# being done\nless dspconfigscript\n# Run the configuration script if it's fine\nbash dspconfigscript\n</code></pre>"},{"location":"dsp/getting-started/basic-setup-network/#intro","title":"Intro","text":"<p>The DSP offers an outgoing proxy giving access to a limited set of resources, but in order to use it some things need to be done in order to use it. Here we go through the most important bits that entails.</p>"},{"location":"dsp/getting-started/basic-setup-network/#certificates","title":"Certificates","text":"<p>To manage what outgoing connections should be allowed and not, the solution needs to inspect outgoing traffic, and to do that, it needs to hijack outgoing connections. To do that, we need clients to trust certificates we issue and thus require clients to import a certificate authority root we provide.</p>"},{"location":"dsp/getting-started/basic-setup-network/#importing-the-ca","title":"Importing the CA","text":"<p>To trust a new CA, we put it under <code>/usr/local/share/ca-certificates/</code> in a file with a prefix of <code>.crt</code> and run <code>update-ca-certificates</code> which makes the system at large trust the certificate.</p>"},{"location":"dsp/getting-started/basic-setup-network/#using-the-ca-for-python-virtual-environments","title":"Using the CA for python virtual environments","text":"<p>Python typically doesn't use the system CA store but rather uses the certifi package to provide the CAs (derived from those used in Mozilla).</p> <pre><code>REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\nexport REQUESTS_CA_BUNDLE\n</code></pre> <p>makes most of those packages use the system CA store instead.</p>"},{"location":"dsp/getting-started/basic-setup-network/#using-the-proxy","title":"Using the proxy","text":"<p>Most tooling will look at the environment variables <code>http_proxy</code>, <code>https_proxy</code> and <code>no_proxy</code> in various uppercase or lowercase combination. Unfortunately, behaviour isn't uniform but mostly</p> <pre><code>http_proxy=https://10.253.254.250:3130/\nhttps_proxy=https://10.253.254.250:3130/\nexport http_proxy\nexport https_proxy\n</code></pre> <p>will do the right thing.</p>"},{"location":"dsp/getting-started/basic-setup-network/#making-docker-use-the-proxy","title":"Making Docker use the proxy","text":"<p>Docker requires some special configuration to use the proxy. Providing</p> <pre><code>[Service]\nEnvironment=\"HTTP_PROXY=https://10.253.254.250:3130/\"\nEnvironment=\"HTTPS_PROXY=https://10.253.254.250:3130/\"\nEnvironment=\"no_proxy=192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,192.168.*,10.*,172.1*\"\n</code></pre> <p>in <code>/etc/systemd/system/docker.service.d/http-proxy.conf</code>, reloading systemd units with <code>sudo systemctl daemon-reload</code> and restarting Docker with <code>sudo systemctl restart docker</code>.</p>"},{"location":"dsp/getting-started/basic-setup-network/#making-apt-use-the-proxy","title":"Making apt use the proxy","text":"<p><code>apt</code> also needs some configuration to use the proxy, putting</p> <pre><code>Acquire::http::Proxy \"https://10.253.254.250:3130/\";\n</code></pre> <p>in e.g. <code>/etc/apt/apt.conf.d/90proxy</code> will cause apt do use the proxy when connecting to the outside world.</p>"},{"location":"dsp/getting-started/exposing-https-services/","title":"Exposing HTTPS services in DSP to the outside","text":""},{"location":"dsp/getting-started/exposing-https-services/#introduction","title":"Introduction","text":"<p>There are use cases where you might want to expose services running in DSP to the outside world. This can be e.g. offering an interactive data explorer or accepting input data for processing and then publishing a report.</p> <p>This guide details services exposed via HTTPS. Other services may be possible to expose but that will involve another, more involved process.</p>"},{"location":"dsp/getting-started/exposing-https-services/#steps","title":"Steps","text":"<ul> <li>allocate a domain name for the service you want to expose</li> <li>wildcard names are not supported</li> <li>point the allocated name to DSP</li> <li>the recommended way to do this is to use a <code>CNAME</code> to     <code>dsp.aida.scilifelab.se</code>, but it's also possible through other means     (details subject to change for now, contact support)</li> <li>DSP will need to request a certificate for the appointed name, this will     be done through Let's Encrypt<ul> <li>for DSP to be able to do this, there can't be any restrictions denying   that (e.g <code>CAA</code> DNS records)</li> </ul> </li> <li>we also support using namespaced names, e.g.   <code>myservice.myproject.dsp.aida.scilifelab.se</code></li> <li>decide on one or more floating IP addresses that will be mapped (\"associated\")   to the VM(s) providing the service</li> <li>any special handling beyond basic up/down detection is currently not     supported (e.g. session handling is not provided)</li> <li>you will need to have the services exposed so that they are reachable from     10.253.254.248/29, i.e. at least one security group allowing the desired     traffic must be added to the instance or the corresponding port</li> <li>contact support with the allocated/desired domain name and the chosen   floating IP address(es)</li> <li>you will receive keys and certificates to use for TLS</li> </ul>"},{"location":"dsp/getting-started/exposing-https-services/#considerations","title":"Considerations","text":"<p>Once done, this will expose your service to the internet at large. It may be appropriate to take precautions, e.g. investigate if the exposed service can be isolated from other things in your secure environment.</p>"},{"location":"dsp/getting-started/first-login/","title":"First login to DSP","text":"<p>While the Data Science Platform tries not to raise to many hurdles, some things are required to make a secure system. Keeping track of who is using it is one such basic thing.</p> <p>When you go to the DSP login page you'll be greeted by a simple screen where you today only have the choice of authenticating with Life Science Login.</p> <p>.</p> <p>If you have not already registered with Life Science login, please follow this guide to do so.</p>"},{"location":"dsp/getting-started/first-login/#back-at-dsp","title":"Back at DSP","text":"<p>Once you have authenticated properly, on the first time you return, you will not be let into DSP as your account needs to be assigned membership details. Instead you will receive the message \"Login failed: You are not authorized for any projects or domains.\"</p> <p></p> <p>Once your account has been adjusted to allow log in, the next time you return you will be let into Horizon.</p> <p></p>"},{"location":"dsp/getting-started/life-science-login/","title":"Life Science federation login","text":"<p>AIDA Data Hub uses the Life Science federated login, a services operated by the European Life Science Research Infrastructures to greatly simplify authentication.</p> <p>When you select to login to our services, you will be taken to the Life Science Login federated authentication, and will be greeted by a screen where you can choose between different services, or type the name of your home organisation to authenticate there if they are available (will likely be the case for academia).</p> <p></p> <p>If your organisation is not available, there are other common services that can be used for authentication (e.g. ORCID, GitHub, LinkedIn, Google, Apple), or if you don't want to or are able to use one of these, you can use \"LifeScience Hostel\" instead which allows signing up at LS Login directly.</p>"},{"location":"dsp/getting-started/life-science-login/#using-your-home-organisation-email","title":"Using your home organisation email","text":"<p>If you use one of these other services, we recommend changing your profile email to the one from your home organisation. This can be done on the LS Login User Profile site; look for \"Preferred mail\" and click the pen to update the mail (note that the new email must be verified which may take some time).</p> <p></p>"},{"location":"dsp/getting-started/life-science-login/#multifactor-authentication","title":"Multifactor authentication","text":"<p>Once logged in, you would normally be passed back to the service that requested authentication. But as our services support working with sensitive data, you will also be required to use multi-factor authentication (somewhat simplified that means just knowing a password should not be enough, typically something more such as having a physical object should be required).</p> <p>With Life Science Login, this means you will be asked to provide a second factor before being able to advance to our services.</p> <p>If Life Science Login needs to do multifactor authentication by itself, you will be sent to the Life Science Login MFA site.</p> <p>For clarity; there's nothing wrong with using a hardware token (e.g. USB key) or other solution, but since the Time-based One-Time Password (TOTP) support is the common denominator, that's shown here.</p> <p>.</p> <p>Since Life Science Login tries to be useful, it supports using modern standards for authentication over the web. Unfortunately, that may mean you get different behaviours depending on what web browser you are using, whatever you have any helper extensions (e.g. password manager), if you have a USB-key connect and possibly even if you have a phone nearby.</p> <p>Since those behaviours differ so much, we won't show them.</p>"},{"location":"dsp/getting-started/life-science-login/#enrolling-a-new-token-for-multifactor-authentication","title":"Enrolling a new token for multifactor authentication","text":"<p>When arriving at the MFA site the first time, it will tell you who you are authenticated as in Life Science Login and inform you that you get in without additional credentials this time.</p> <p></p> <p>You will then be guided through the enrollment flow, starting with the ability to name your token.</p> <p></p> <p>Next, the flow tries to ensure you have a TOTP application available. It suggests alternatives for Android and iOS, but you don't need to use those applications as TOTP is a standard. The list of working apps include Twilio Authy, Google Authenticator, Microsoft Authenticator, FortiToken, Duo security authentication, common password managers and many more.</p> <p>If you do not already use two-factor authentication with an app, we strongly recommend choosing one that supports some kind of secure network synchronisation to handle cases where your device stops working (this should do encryption on device).</p> <p></p> <p>Once you have confirmed you have an app, it will show you a QR code for easy addition of the second factor. If you are using a phone, you should be able to scan that and hopefully have the account added automatically.</p> <p>If not, you can ask it to reveal the secret as text and add that to your MFA solution manually.</p> <p>Once you have added the account to your solution, it should present you with the current code (this will update regularly, at any time it is requested you should enter what is currently shown).</p> <p></p> <p>If that works, it will remember that you have registered the account on your side. It calls with that the \"token is enrolled\"</p> <p></p> <p>It will then let you know that multifactor authentication has been activated.</p> <p></p> <p>Next, it will inform you about backup codes (in case you loose your phone or similar). Even if you have a solution that helps with network backup, downloading backup codes and storing them securely is strongly recommended.</p> <p></p> <p>Once you've done that, MFA activation is complete.</p> <p></p>"},{"location":"dsp/getting-started/life-science-login/#attribute-release","title":"Attribute release","text":"<p>To operate our services, we need some additional details about you from Life Science Login. For them to be able to pass them to us, they need you to acknowledge that:</p> <p>Click \"Yes, continue\" to let Life Science Login give us those details. If you want to, you can choose \"Remember\" not to have to click through that screen the next time.</p> <p></p>"},{"location":"dsp/getting-started/life-science-login/#back-to-aida-data-hub-services","title":"Back to AIDA Data Hub services","text":"<p>Once you've registered and accepted that our services can get additional details, you should be redirected back to the service for which you originally initiated the log in. You will now be able to use the same Life Science login profile for all of the AIDA Data Hub services (as well as any other of the European research infrastructures using it).</p>"},{"location":"dsp/getting-started/vm-access/","title":"Setting up and logging into a machine in DSP","text":"<p>This guide assumes you have access to the Horizon interface, if not, see the first login guide.</p>"},{"location":"dsp/getting-started/vm-access/#some-terminology","title":"Some terminology","text":"<p>Some terms used in OpenStack may need some explanation:</p> <ul> <li>an instance means an actual virtual machine (VM), it can be turned off (in   state <code>shutdown</code>) or running, spawning and so on</li> <li>a flavor is a resource specification for a type of virtual machine, e.g.   say that a small machine can have 1 CPU core, 1 GByte of RAM and no disk)</li> <li>a volume is a virtual disk</li> <li>a security group is how OpenStack manages traffic filtering for a machine. In   OpenStack all traffic is disallowed unless there is a rule in a security group   attached to the machine specifying that the specific traffic is allowed. This   can be done with various filters (e.g. IP addresses, port, direction and so   on)</li> <li>ingress mean incoming</li> <li>egress mean outgoing</li> <li>a floating ip is a virtual IP address that can be assigned to a machine to   make it reachable at that address, but in contrast to \"regular\" addresses,   these will typically not show up on the machine and can also be removed and   reassigned at any time</li> <li>server groups is a way of grouping virtual machines together for purposes of   scheduling in openstack (\"affinity\"), it can be used to say that some virtual   machines should run on the same physical host (to offer higher bandwidth and   lower latency for things that talk to a lot) or on different physical hosts   to add fault-tolerance</li> </ul>"},{"location":"dsp/getting-started/vm-access/#pecularities-for-dsp","title":"Pecularities for DSP","text":"<p>Since DSP aims to support research on sensitive data, there a a few things that differ from what you'd normally encounter in a cloud setup.</p> <p>The most important is that there is no way to connect to virtual machines directly. It's quite common that virtual machines use internal networking and can be connected to the outside world by assigning an IP from a \"public\" network.</p> <p>This can be done within DSP as well, but the public network is completely internal to the DSP, so exposing a machine on the public network in DSP lets you connect to it through our gateway or from other DSP projects if your security groups allow it.</p>"},{"location":"dsp/getting-started/vm-access/#preparations","title":"Preparations","text":"<p>To be able to access any virtual machines you create, you will need to be able to authenticate to it. In practice, this means you should upload the public part of a SSH key pair so it can be preloaded on machines you create.</p> <p>This is accessible in Horizon under Compute \u082c Key Pairs where you can see your keys and import or create new ones.</p> <p></p> <p>On the key pairs screen, your known key pairs will be listed and you have buttons to create a new key pair (you probably don't want to use this), import a key and delete a key.</p> <p></p> <p>Click the \"Import Public Key\" to open the key importer.</p> <p></p> <p>Choose a name for your key, select the type (SSH Key) and either upload or copy paste the public part of your key pair (authentication depends on you proving you have the private part which can be verified by someone that has the public part).</p>"},{"location":"dsp/getting-started/vm-access/#creating-a-vm","title":"Creating a VM","text":"<p>Now, you can actually create a VM, go to the Instances screen (Compute \u082c Instances).</p> <p></p> <p>Similarly as for key pairs, you get to see a list of resources (instances) and have a few buttons for quick access.</p> <p></p> <p>Clicking the \"Launch Instance\" buttons brings up the instance launcher which will guide you through the instance creation. While it has a guided flow, you jump around as you please. You can see things you must take care of being marked with an asterisk (*) in the left pane.</p> <p>On the first screen, you enter a name and a description for your VM.</p> <p>Once done, advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>On the second screen, you need to choose what to start. The \"Select Boot Source\" lets you choose between Image (can be said to be a prepared VM), \"Instance Snapshot\" (a snapshot you've taken from a VM previously), \"Volume\" (a virtual disk) or \"Volume Snapshot\".</p> <p>Typically you will be using Image unless you have a special use case.</p> <p>To choose what to start from, click the up arrow button to the right in the list of available resources. We'll use the \"Ubuntu Noble server\" image for this demonstration.</p> <p>Once done, advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next, you'll need to choose the flavor to use. A flavor in OpenStack describes the resources (memory, cores, disk and such, GPUs) allocated to the VM. Same as for the Image, choosing one is done by using the arrow up button to the right in the list.</p> <p>Once this is done, you can normally start your VM by using \"Launch Instance\" at the lower right (it should activate once you choose a flavor), but for this guide we'll go through the other possible steps.</p> <p>If you choose Image or Instance snapshot as source, you have the option of creating a new volume (virtual disk) for it, and if so, what size it should have (if you need more than the default suggested by the image) and if it should be automatically deleted when you remove your virtual machine.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next up is the networks screen which allows e.g. choosing what virtual networks are connected if you have access to many. It's unlikely you will want to do anything there for now.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next up is the network ports screen which allows e.g. choosing what virtual network connectors that are used if you have access to many. You probably don't need to do anything here..</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Security groups determine what security groups are attached to the virtual machine. A machine can have any number of security groups attached, each attached will potentially allow additional traffic to/from the VM.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next up is the key pair screen which lets you choose which key pair to preload. Same as with other screens, you can select one by clicking the arrow up button.</p> <p>At most one can be selected at the same time.</p> <p>The selected key pair will be offered as information to the virtual machine, there's no guarantee it's used (e.g. an instance snapshot might not care about) it at all.</p> <p>For virtual machines images intended to be used to create virtual machines, this will typically be assigned to the default user for the image. For e.g. an Ubuntu image, it will be added to the <code>ubuntu</code> user.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>The technology used to offer the key pair chosen above can do a lot more - see the cloud-init documentation for more informtation.</p> <p>The Configuration screen allows you to provide more configuration. That can be used to e.g. pre-create some additional users, set a password, partition drives or other things.</p> <p>This is an advanced future that will not be used in this guide.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next up is the server group screen where you can decide which (if any) server groups a virtual machine should belong to. These will be used to decide how scheduling happens.</p> <p>This is an advanced future that will not be used in this guide.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Next up is the scheduler hints screen where you can provide hints for the scheduler.</p> <p>This is an advanced future that will not be used in this guide.</p> <p>Advance by clicking the \"Next\" button to the lower right.</p> <p></p> <p>Finally the metadata screen allows you assign metadata to the virtual machine, either from a list of preloaded suggestions or custom keys (once added to your virtual machine you can provide values).</p> <p>Since we've reached the end of the guided experience, we'll create our virtual machine by clicking \"Launch Instance\".</p> <p></p> <p>After a short while, you'll be taken back to the Instance view where after some time your new virtual machine should pop up in the list. It will go through various stages but unless there is an issue, it should end up in Power state \"Running\", meaning it's powered on.</p> <p>You have a button for quick actions to the right or you can click your machine to go into the details screen for it.</p> <p></p> <p>After clicking your virtual machine to see more details, you will be presented with a view with more information in various tab, e.g. an overview.</p> <p></p> <p>There is also a tab for the machine log that can be useful when troubleshooting.</p> <p></p> <p>And there is a console tab for emergency access to the \"graphical console\" of the virtual machine. This is not indented to be used for a virtual desktop but rather for emergency cases (e.g. while it was stopped you disconnected and deleted a volume that is no longer needed but is expected to be mounted at start).</p> <p>Since we're happy with the virtual machine, we click the actions menu button to the upper right to fold out the menu.</p> <p>Some operations that are considered especially dangerous is marked by using another text color (red instead of black here).</p> <p></p> <p>Since we want to reach our VM from the outside, we'll go ahead and click \"Associate floating IP\" from the list.</p> <p></p> <p>There will be a drop-down list of addresses to choose from at IP address. We pick one at random.</p> <p>Port to be associated will be filled in correctly for most cases, so we disregard that for now. and click \"Associate\".</p> <p></p> <p>Once we've chosen, the new address will be visible in the overview tab as well as in the virtual machine list on the main Instances screen.</p> <p>But before we're done, we also need to actually make the virtual machine accessible, this is done by adding security groups that allow the traffic we want.</p> <p>To do this, we use the actions drop-down menu to select \"Edit Security Groups\". Note that there is currently a bug in the graphical interface so that active groups aren't shown correctly, but that doesn't stop us here, for right now, let's add all groups.</p> <p>Once we're done, we chose \"Save\" to make the changes happen.</p> <p></p>"},{"location":"dsp/getting-started/vm-access/#connecting-to-the-virtual-machine","title":"Connecting to the virtual machine","text":"<p>Now, we can finally try to connect to our new virtual machine. To do so, we need to use the floating IP we associated with the machine earlier.</p> <p>In this case, it's <code>10.253.16.34</code>.</p> <p>Since it's the most common and versatile, we'll use the standard OpenSSH SSH client here, but similar functionality should be achievable with other clients.</p> <p>As quick a quick connection without doing any configuration, we can connect with</p> <pre><code>ssh -o \"ProxyJump your.email@example.com@dsp.aida.scilifelab.se\" ubuntu@10.253.16.34\n</code></pre> <p>where <code>your.email@example.com</code> is replaced by your actual email address used for DSP. There will be a lot of <code>@</code> characters on that line, but it's fine.</p> <p>Running that command will probably ask you to about the key the first time. It's a good habit to verify unknown keys, so we should do that. The key for DSP has fingerprint as below.</p> <pre><code>256 SHA256:vg/InkHbVMb3FEyouth7f+WLi1OtEKgOJ88q49fVmj0 dsp (ED25519)\n+--[ED25519 256]--+\n|             +o  |\n|         .  . .. |\n|          =.. .  |\n|         +.o o   |\n|      . S.. . o  |\n|     = O.. o . ..|\n|      &amp;o=.+ o .o |\n|   o o.Bo=.E..o .|\n|  ..+.+ o+.o++...|\n+----[SHA256]-----+\n</code></pre> <p>Once you've checked and approved the key (the ), you will be shown a banner with a link.</p> <p></p> <p>Clicking that link will prompt you to go through with authentication through Life Science Login. If you go through that, you should end up on a page with a short message:</p> <pre><code>You are now logged in as &lt;your.email@example.com&gt; and should be able to continue in your ssh session.\n</code></pre> <p>(where <code>your.email@example.com</code> should be your actual email used for DSP).</p> <p>Once you get that, you can go back to the terminal, pressing return there should advance and allow your SSH client to connect your actual virtual machine.</p> <p>But since it's a completely new machine, we don't have a way of verifying the public key for it, so we'll need to go by faith here.</p> <p>It'a also possible that you get problems here if you create, delete and create many virtual machines and reuse the floating IPs to connect to, in that case <code>ssh</code> will help by suggesting commands to clean up conflicting details from your files.</p> <p></p> <p>Accepting that public key will allow us to advance. Since we've given the virtual machine our public key already, it can authenticate us and let us in.</p> <p></p> <p>It is good practice to protect keys with a passphrase, but I didn't need to type any here. That's because I use an agent to help with the keys so that I can add them once at login by typing the passphrase and then don't need to type it anymore. This is available with the <code>ssh-agent</code> tool, but also built-in to many common desktop environments, you can try e.g. <code>ssh-add -L</code> to list identities your agent know of, and if that works (doesn't give an error message) you can add your identities by e.g. <code>ssh-add ~/.ssh/mykeyfile</code> to add a specific key or just <code>ssh-add</code> to go through the default key files OpenSSH will look for.</p>"},{"location":"dsp/getting-started/vm-access/#nicer-configuration","title":"Nicer configuration","text":"<p>While it works to do as above, it can be tiresome having to click links to connect. Fortunately, OpenSSH offers way to improve the experience.</p> <p>To get out of clicking the link, we can use something called <code>connection multiplexing</code> which allows SSH to use a single connection to many different things. This happens after authentication, so activating it for the gateway means we can use it to connect multiple times.</p> <p>Since this means authentication does not happen again, it's something to consider and necessitate some precautions such as automatic locking when idle and so on (but such precautions should typically be in place already if you work with sensitive data).</p> <p>The configuration below asks SSH that new connections should automatically set up multiplexing and hang around after use, but stop after one half hour. There are also other options for <code>ControlMaster</code>, allowing for requesting confirmation before using the multiplexing, see the manual page <code>ssh_config(5)</code>.</p> <pre><code>ControlMaster auto\nControlPersist 1800\nControlPath ~/.ssh/socket-%r@%h-%p\n</code></pre> <p>OpenSSH also allows us to define things so we don't need to type it so much.</p> <pre><code>Host dspgateway\n  Hostname dsp.aida.scilifelab.se\n  User your.email@example.com\n</code></pre> <p>(where <code>your.email@example.com</code> should be replaced by your actual email used for DSP.)</p> <p>And OpenSSH also offers the <code>ProxyJump</code> feature to automatically use tunneling through a host, we can set this up for our VMs as such</p> <pre><code>Host 10.253.*\n  ProxyJump dspgateway\n  User ubuntu\n</code></pre> <p>combining these, we can put it in a file (e.g. <code>~/.ssh/dsp_config</code>) and add an include statement for it in the main SSH configuration (<code>~/.ssh/config</code>) such as</p> <pre><code>Include dsp_config\n</code></pre> <p>with <code>dsp_config</code> consisting of</p> <pre><code>ControlMaster auto\nControlPersist 1800\nControlPath ~/.ssh/socket-%r@%h-%p\n\nHost dspgateway\n  Hostname dsp.aida.scilifelab.se\n  User your.email@example.com\n\nHost 10.253.*\n  ProxyJump dspgateway\n  User ubuntu\n</code></pre> <p>I can just use the IP directly:</p> <pre><code>$ ssh 10.253.16.34\nLast login: Wed Feb  5 11:24:49 2025 from 10.253.254.251\nTo run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\".\nSee \"man sudo_root\" for details.\n\nubuntu@mytestmachine:~$\n</code></pre> <p>(notice that I get a different view here with no message-of-the-day because I'm already logged in.)</p>"},{"location":"dsp/getting-started/storage/","title":"Introduction to storage","text":"<p>Storage is used to refer to functionality to store or persist (for some amount of time) information in various forms.</p> <p>It's common practice to refer to different kinds of storage services:</p> <ul> <li>block storage</li> <li>object storage</li> <li>file storage</li> </ul>"},{"location":"dsp/getting-started/storage/#block-storage","title":"Block storage","text":"<p>Block storage refers to where storage is offered as a larger area (block) of data that can be used. One example of this is a hard drive or USB stick which when plugged in will make its storage available as one area.</p> <p>For cloud environments, block storage is typically offered through some form of volume which can be thought of as a virtual hard disk that can be connected (attached) to virtual machines. While these are typically provided over a network, they normally show up as some form of local device in the virtual machine where they are attached.</p> <p>As hard disks are seldom connected to multiple computers, most uses assume they have full control of the device and so is also the case for block storage, meaning it can normally not be connected to more than one machine at the same time.</p> <p>Just like hard disks, block storage (volumes) typically need some form of initialisation (formatting) to be usable.</p>"},{"location":"dsp/getting-started/storage/#object-storage","title":"Object storage","text":"<p>Object storage (sometimes called blob storage) can be used from multiple points simultaneously and doesn't require as tight connection between the place where the data is used and the data is stored.</p> <p>Object storage on DSP is described in more detail here.</p>"},{"location":"dsp/getting-started/storage/#mounting","title":"Mounting","text":"<p>It's good to note that there exists many solution to expose an object store as a file system. While this can be very practical, it can behave slightly different from what would be expected from a file system (e.g. what happens if files are renamed and so on).</p>"},{"location":"dsp/getting-started/storage/#file-storage","title":"File storage","text":"<p>File storage is normally offered as a network file system. Like block storage, this typically requires the place where data is stored and where the data is used, but in contrast to block storage, file storage can normally be accessed from multiple places simultaenously.</p> <p>File service is not currently offered on DSP, but it's a planned feature.</p>"},{"location":"dsp/getting-started/storage/object-store/","title":"Using object storage at DSP","text":""},{"location":"dsp/getting-started/storage/object-store/#introduction-and-terminology","title":"Introduction and terminology","text":"<p>DSP offers object storage through Ceph. This enables access through different protocols, including s3 (as offered by AWS among others) and Swift. This means that you can use a wide variety of clients to connect to the offered services.</p> <p>Unfortunately, these services use different names for similar concepts - s3 use the term bucket whereas swift uses container. Since it's the more widely used, this document will use the s3 terminology of buckets, which should be understandable in most cases (and is somewhat less prone to confusion whereas container can refer to other concepts).</p>"},{"location":"dsp/getting-started/storage/object-store/#web-interface","title":"Web interface","text":"<p>The cloud web interface (OpenStack Horizon) offers a built-in browser where you can create buckets, browse existing buckets as well as upload and download data.</p> <p>This browser is available in Horizon under Project -&gt; Object Store -&gt; Containers.</p> <p></p>"},{"location":"dsp/getting-started/storage/object-store/#accessing-via-other-means","title":"Accessing via other means","text":""},{"location":"dsp/getting-started/storage/object-store/#openstack-client","title":"openstack client","text":"<p>The <code>openstack</code> command-line tool allows access through its <code>object</code> and <code>container</code> subcommand classes (assuming valid authentication, typically through application credentials).</p> <pre><code>user@host:~$ openstack container list\n+---------------+\n| Name          |\n+---------------+\n| test          |\n| important     |\n| stuff         |\n+---------------+\nuser@host:~$\n</code></pre> <pre><code>user@host:~$ openstack object list test\n+-----------------------------+\n| Name                        |\n+-----------------------------+\n| some-data                   |\n| something-else              |\n| some/structure              |\n+-----------------------------+\nuser@host:~$\n\n</code></pre>"},{"location":"dsp/getting-started/storage/object-store/#creating-credentials-for-s3-access","title":"Creating credentials for s3 access","text":"<p>Access through s3 protocol is also possible, but likely requires acquiring a separate set of credentials (\"ec2 credentials\").</p> <p>Managing these is available in the Horizon web interface under Project -&gt; API Access, where \"View Credentials\" will trigger creation of credentials usable for the currently selected project (secure environment).</p> <p>Once credentials are created, theree will also be a button \"Recreate EC2 Credentials\".</p> <p></p> <p></p> <p>[!IMPORTANT] For security reasons, we will start enforcing a finite life span for credentials for S3 (EC2 credentials). The process for how this will be done is still to be determined, though.</p>"},{"location":"dsp/getting-started/storage/object-store/#using-s3-clients","title":"Using s3 clients","text":""},{"location":"dsp/getting-started/storage/object-store/#s3cmd","title":"s3cmd","text":"<p>s3cmd is client fairly common (for historical reasons if nothing else).</p> <p>After creating credentials, you can make a configuration such as</p> <pre><code>[default]\nhost_bucket = s3.dsp.aida.scilifelab.se\nhost_base = s3.dsp.aida.scilifelab.se\naccess_key = YOURACCESSKEY\nsecret_key = YOURSECRETKEY\n</code></pre> <p>replacing <code>YOURACCESSKEY</code> and <code>YOURSECRETKEY</code> with those from your credentials, respectively.</p> <p>If you store that in e.g. <code>s3cmd-dsp.conf</code>, you can then use <code>s3cmd</code> such as</p> <pre><code>user@host:~$ s3cmd -c s3cmd-dsp.conf ls s3:///\n2025-09-11 07:53  s3://test\n2025-03-21 12:02  s3://another\nuser@host:~$ s3cmd -c s3cmd-dsp.conf ls s3://test/\n                          DIR  s3://test/scripts/\n2025-04-28 18:02       341976  s3://test/logo.png\nuser@host:~$\n</code></pre>"},{"location":"dsp/getting-started/storage/object-store/#s3fs","title":"s3fs","text":"<p>If you are on a system where it's suppored, you can mount buckets using one of many fuse-solutions, here's one called s3fs shown (mounting the bucket <code>another</code> to the directory <code>data</code>).</p> <pre><code>user@host:~$ export AWS_ACCESS_KEY_ID=YOURACCESSKEY\nuser@host:~$ export AWS_SECRET_ACCESS_KEY=YOURSECRETKEY\nuser@host:~$ mkdir -p data\nuser@host:~$ s3fs -o url=https://s3.dsp.aida.scilifelab.se/ another data\nuser@host:~$ ls data\nbin lib test.py\nuser@host:~$ umount -l data\n</code></pre>"},{"location":"dsp/getting-started/storage/object-store/#access-control","title":"Access control","text":"<p>There is a built-in system for access control that allows for fine grained management of rights. We expect to functionality for offering presets (e.g. \"allow internal access only\"). For now this needs to be managed manually by the user though.</p>"}]}